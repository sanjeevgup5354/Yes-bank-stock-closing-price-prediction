{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjeevgup5354/Yes-bank-stock-closing-price-prediction/blob/main/Yes_Bank_Stock_Closing_Price_Prediction_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Yes Bank Stock Closing Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Sanjeev Kumar Gupta\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to analyze the impact of a fraud case involving Rana Kapoor on the stock prices of Yes Bank, a prominent bank in the Indian financial domain. The dataset used in this project consisted of monthly stock prices of Yes Bank since its inception, including closing, starting, highest, and lowest stock prices.\n",
        "\n",
        "To predict the stock's closing price, I developed two models namely Linear regression and Random Forest  model was developed. The model was trained using the historical stock price data and various features such as mean of Open, High and Low faetures.Additional features were engineered by taking mean of Open, High and Low.The performance of the model was evaluated using metrics like RMSE (Root Mean Squared Error),adjustes R2 and R2 score. Random Forest model performed well among the three with high R2 score and adjusted R2.\n",
        "\n",
        "The analysis aimed to uncover any patterns or changes in stock prices related to the fraud case involving Rana Kapoor. The feature importance provided by the Random forest model helped identify the key factors influencing the stock price.\n",
        "\n",
        "Overall, the project aimed to contribute to a better understanding of the relationship between the fraud case and Yes Bank's stock prices, and to explore the potential of predictive models in the financial domain. The findings and insights gained from this project can be utilized by investors, analysts, and decision-makers to make informed investment or business decisions related to Yes Bank's stock."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project description**\n",
        "\n",
        "Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations. This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month. The main objective is to predict the stockâ€™s closing price of the month.\n",
        "\n",
        "\n",
        "**Business Objective**\n",
        "\n",
        "To develop a robust predictive model capable of accurately forecasting the monthly closing stock prices of Yes Bank, leveraging historical stock data. This initiative aims to assist stakeholders in making informed investment decisions by providing reliable predictions amidst the backdrop of the bank's turbulent operational environment, marked by the high-profile fraud case involving Rana Kapoor since 2018. By employing advanced machnie learning modeling techniques and predictive algorithms, the project seeks to mitigate uncertainty and enhance the ability to anticipate future market trends, thereby enabling stakeholders to optimize their investment strategies and maximize returns."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Dataset\n",
        "\n",
        "stock_df=pd.read_csv('/content/sample_data/data_YesBank_StockPrices.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "stock_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.tail()"
      ],
      "metadata": {
        "id": "Y6J6eaubuml-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "stock_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "stock_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "stock_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "stock_df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(stock_df.isnull())"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consist of 185 rows and 5 columns. The dataset consist of data ranging from July 2005 to November 2020. The dataset consist of opening price, closing price, highest and lowest stock price for the particular months.\n",
        "There are no missing and duplicate values found in the dataset.\n",
        "There are four independent varibale(date, open, high,low) and one dependent(close) variable found."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "stock_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "stock_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Date** : Date of record\n",
        "\n",
        "**Open** : Opening price of stock for the month(numerical)\n",
        "\n",
        "**High** : Highest price for the month(Numerical)\n",
        "\n",
        "**Low** : Lowest price for the month(Numericl)\n",
        "\n",
        "**Close** : Closing price for the month(Numerical)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "columns=stock_df.columns\n",
        "for i in columns:\n",
        "  print('Unique values in',i,'are:')\n",
        "  print(stock_df[i].unique())\n",
        "  print()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a copy of original dataset\n",
        "df=stock_df.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting datatype of DAte from object to datetime\n",
        "df['Date']=pd.to_datetime(df['Date'].apply(lambda x: datetime.strptime(x,'%b-%y')))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IDMzLpGEFfhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning date column as index\n",
        "df.set_index('Date',inplace=True)\n"
      ],
      "metadata": {
        "id": "WyNmhkSoNyHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(df)\n"
      ],
      "metadata": {
        "id": "lpNzfglN-CmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "jPSXe5PHO0Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "kugU_q7PKi4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "klxsSCrTG0MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manipulations**\n",
        "\n",
        "Created a duplicate copy of original dataset to make changes over the dataset.\n",
        "Changed the data type of Date column from object to datetime. There are no duplicate and null values found in the dataset.\n",
        "Assigned date column as the index of the dataset.\n",
        "\n",
        "**Insights**\n",
        "\n",
        "The disributuion is skewed as there is huge difference between 75 percentile and max vale of the dataset."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Yearly opening price"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(df.index,df['Open'])\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Opening Price')\n",
        "plt.title('Yearly Opening Price')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: To show to yearly distribution of stock prices"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: After seeing the graph it is clear that after the fraud case happened in 2018 there is huge fall in opening price"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: After reviewing the chart itis clear that with the impact of fraud case there is huge fall in the opening price of stock price so this should be kept in mind for future references."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - yearly closing price"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# closing price distrubution over the years\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(df['Close'])\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Yearly Closing Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : To viw the distribution of closing price over the years"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: After the 2018 incident there is a huge fall in the closing price  which resulted in a huge market loss."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answee: After reviweing the graph one can say that there is a huge loss in the after yers of 2018 due to the 2018 case so this should bekept in mind for future references."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Distribution of closing price"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# density distribution of closing price(dependent variable)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.distplot(df['Close'])\n",
        "plt.title('Distribution of Closing Price')\n",
        "plt.axvline(df['Close'].mean(),color='red')\n",
        "plt.axvline(df['Close'].median(),color='yellow')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : To review the density distribution of closing price to decide for further treatment or transformation of data."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: After reviewing the chart it is clear that there is a skewed distribution in the chart."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: To buid a perfecct machine learning model we need to transfor the data so that the machine can predict the output with a high accuracy."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Distribution of opening price"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Density plot for the distribution of opening prices\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.distplot(df['Open'])\n",
        "plt.title('Distribution of Opening Price')\n",
        "plt.axvline(df['Open'].mean(),color='red')\n",
        "plt.axvline(df['Open'].median(),color='yellow')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: To check the density distribution of opening price"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The chart is skewed toward the left or we can say that the data is deviated from its mean."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: To build a model which can pridict future stock prices with high level of accuracy we need to transform that data so it can acquire a normal distrubution."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Distribution of Highest and lowest price"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Density distribution of Highest and lowest price\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.distplot(df['High'])\n",
        "plt.title('Distribution of Highest Price')\n",
        "plt.axvline(df['High'].mean(),color='red')\n",
        "plt.axvline(df['High'].median(),color='yellow')\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.distplot(df['Low'])\n",
        "plt.title('Distribution of Lowest Price')\n",
        "plt.axvline(df['Low'].mean(),color='red')\n",
        "plt.axvline(df['Low'].median(),color='yellow')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer To show the density distribution of highest and lowest prices of stocks with in the data."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: After viewing the charts we found that both the highest and lowest price are poistively skewed i.e. they are skewed towards right."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: we have to treat and trandsform the data to convert it into normal distribution to train the machine learning model with the best set of data so we can get the prediction with high level of accuracy."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Trend of closing price over time"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates = df.index\n",
        "closing_prices = df['Close']\n",
        "\n",
        "# Create a line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(dates, closing_prices)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Trend of Closing Prices over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: To ceheck the trend of closing prices over time."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The trend shows that there is a gradual decrease in the closing price after 2018 when the fraud case happened."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart -Candle stick chart\n"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CAndle stick chart\n",
        "!pip install mplfinance\n",
        "\n",
        "\n",
        "import mplfinance as mpf\n",
        "\n",
        "df_candle = df[['Open', 'High', 'Low', 'Close']]\n",
        "mpf.plot(df_candle, type='candle', style='yahoo', title='Candlestick Chart', xlabel='Date')\n"
      ],
      "metadata": {
        "id": "w1S5k7l6SufO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "Answer: candle sticks are most widely used finance world. with this chart I wnat to see how variables are each month."
      ],
      "metadata": {
        "id": "xq5p-4OeSsX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "Answer: from the above chart, there is no much deviation from the price shown by open, high, low.\n"
      ],
      "metadata": {
        "id": "xByjsJkAT65K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Answer: Ii is important to make sure that there is no huge variation in the data."
      ],
      "metadata": {
        "id": "1oZ3Bfb2T3Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Chart - box plot for independent variables"
      ],
      "metadata": {
        "id": "57Hy2BalUry5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(data=df)\n",
        "plt.title('Boxplot for Independent Variables')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Plotting a boxplot is a good way to find outliers present in the data. In the same way we had plotted a boxplot to check for the outliers present in the data."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: It can be clearly seen that there are outliers present in the data."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The present outlires need to be treated as it will let to baised predition in the machnie learning maodel which is not good as per business point of view."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart- Correlation heatmap"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(df.corr(),annot=True)\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "FnX0AOdinotL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : To chec the correlation between different variables of the dataset."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: From the heatmap it is clear that eac varibalr is highly correlated to another variable."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Pair plot"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair plot\n",
        "sns.pairplot(stock_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Seaborn Pairplot allows us to plot pairwise relationships between variables within a dataset. It gives us in single large picture. This is used essentially to get to know about our data and how our target variable is related with the rest of the variables"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Each variable is correlated with another variable and follows a linear distribution."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement1**: There is a significant difference in the mean closing prices between the first half (2005-2017) and the second half (2018-2020) of the dataset.\n",
        "\n",
        "**Statement2**: There is a significant difference in the mean closing prices between months with high opening prices and months with low opening prices."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null hypothesis(H0)**: There is no significant difference in the mean closing prices between the first half (2005-2017) and the second half (2018-2020) of the dataset.\n",
        "\n",
        "**Alternative hypothesis(H1)**:There is a significant difference in the mean closing prices between months with high opening prices and months with low opening prices."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Create a new copy of the dataset\n",
        "df1=df.copy()\n",
        "\n",
        "# Divide the dataset in two part before2017 and after 2018\n",
        "before2017=df1['Close'][df1.index<'2017-01-01']\n",
        "after2018=df1['Close'][df1.index>'2018-01-01']\n",
        "\n",
        "# calculating mean  and standard deviation for both datasets\n",
        "mean_before2017=before2017.mean()\n",
        "mean_after2018=after2018.mean()\n",
        "std_before2017=before2017.std()\n",
        "std_after2018=after2018.std()\n",
        "\n",
        "# calculate the sample size\n",
        "n1=len(before2017)\n",
        "n2=len(after2018)\n",
        "\n",
        "# Calculate the standard error of the difference between means\n",
        "standard_error = np.sqrt((std_before2017**2 / n1) + (std_after2018**2 / n2))\n",
        "\n",
        "# Calculate the z-score\n",
        "z = (mean_before2017 - mean_after2018) / standard_error\n",
        "\n",
        "# Calculate the p-value (two-tailed test)\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare the p-value with the significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in means.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in means.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : To check the P-value we had performed z-test as the data contains more than 30 entries"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer; The datast consist of more than 30 entried thats why we had performed z-test."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null hypothesis(H0)**: The mean closing prices in months with high opening prices are equal to or lower than the mean closing prices in months with low opening prices.\n",
        "\n",
        "**Alternative hypothesis(H1)**:There is a significant difference in the mean closing prices between months with high opening prices and months with low opening prices."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Calculate the mean opening and closing prices for each month\n",
        "df1['Month'] = df1.index.strftime('%Y-%m')\n",
        "monthly_data = df1.groupby('Month').agg({'Open': 'mean', 'Close': 'mean'})\n",
        "\n",
        "# Define the threshold for high and low opening prices\n",
        "threshold = monthly_data['Open'].median()\n",
        "\n",
        "# Divide the data into groups based on opening prices\n",
        "high_opening_prices = monthly_data[monthly_data['Open'] > threshold]['Close']\n",
        "low_opening_prices = monthly_data[monthly_data['Open'] <= threshold]['Close']\n",
        "\n",
        "# Calculate the sample statistics\n",
        "mean_high = np.mean(high_opening_prices)\n",
        "mean_low = np.mean(low_opening_prices)\n",
        "std_high = np.std(high_opening_prices)\n",
        "std_low = np.std(low_opening_prices)\n",
        "n_high = len(high_opening_prices)\n",
        "n_low = len(low_opening_prices)\n",
        "\n",
        "# Calculate the z-statistic\n",
        "z_statistic = (mean_high - mean_low) / np.sqrt((std_high**2 / n_high) + (std_low**2 / n_low))\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 1 - stats.norm.cdf(z_statistic)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare the p-value with the significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in the mean closing prices.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in the mean closing prices.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t-test\n",
        "# Divide the data into groups based on opening prices\n",
        "high_opening_prices = df1[df1['Open'] > threshold]['Close']\n",
        "low_opening_prices = df1[df1['Open'] <= threshold]['Close']\n",
        "\n",
        "# Perform independent t-test\n",
        "t_statistic, p_value = stats.ttest_ind(high_opening_prices, low_opening_prices)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare the p-value with the significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in the mean closing prices.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in the mean closing prices.\")\n"
      ],
      "metadata": {
        "id": "11ZcOxm7d1KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: t-test and z-test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: As our data is not noermally distrinuted so we had choosen t-test and z-test."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new copy of the original dataset\n",
        "df2=df.copy()"
      ],
      "metadata": {
        "id": "YVOyKVnuXQK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df2.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling data type of variables\n",
        "df2.dtypes"
      ],
      "metadata": {
        "id": "HmQtA9J6V7Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: There are no missing values present in the dataset so no need for imputation."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Outliers will be handled during data transformation and scaling"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "# There are no need for categorical encoding as no categorial data is present in the dataset"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Adding new feature mean_ohl\n",
        "df2['mean_ohl'] = (df2['Open'] + df2['High'] + df2['Low'])/3\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "x_independent=df2['mean_ohl']\n",
        "y_dependent=df2['Close']"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: As we have only few features aviable so we had selected one new feature mean_ohl as inpdependent variables which is the mean of Open, High and Low and Close as dependent variable"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: I had found all the feture important due to less number of feature availablity."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "# As our data is skewed towards positive direction so we need to transform our data to remove the skewness from the data after which the data will acquire\n",
        "#normal distributuion.\n",
        "\n",
        "# Tranforming mean_ohl and close using log transformation\n",
        "trans_log=FunctionTransformer(np.log1p)\n",
        "df2['mean_ohl']=trans_log.fit_transform(df2['mean_ohl'])\n",
        "df2['Close']=trans_log.fit_transform(df2['Close'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "scaler=StandardScaler()\n",
        "df2[['mean_ohl']]=scaler.fit_transform(df2[['mean_ohl']])\n",
        "\n",
        "# dataset after transformation and scaling\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifying the value to the variables\n",
        "x_independent=df2[['mean_ohl']]\n",
        "y_dependent=df2['Close']"
      ],
      "metadata": {
        "id": "4kS3x5z0dQCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the relationship between dependent and independent variables\n",
        "sns.lmplot(x='mean_ohl', y='Close', fit_reg=True, data=df2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xWDY0toXeVXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here: As the data is not large and not many columns are available so i dont think dimentionality reduction is necessary."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x_train,x_test,y_train,y_test=train_test_split(x_independent,y_dependent,test_size=0.2,random_state=42)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The training to test data ration is 4:1."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**The data is normally spread and there is no imbalence in the dataset so no modification needed."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - Regression model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = model.predict(x_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual data and predicted data\n",
        "pred_data=pd.DataFrame({'Actual':np.exp(y_test),'Predicted':np.exp(y_pred)})\n",
        "pred_data.head()"
      ],
      "metadata": {
        "id": "3FecpMH9mGhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluatin metrics form regression model\n",
        "\n",
        "\n",
        "print('tarining data metrics:')\n",
        "train_MAE=mean_absolute_error(y_train,model.predict(x_train))\n",
        "print(f\"Mean Absolute Error train : {train_MAE}\")\n",
        "\n",
        "train_MSE=mean_squared_error(y_train,model.predict(x_train))\n",
        "print(f\"Mean Squared Error train : {train_MSE}\")\n",
        "\n",
        "train_RMSE=np.sqrt(train_MSE)\n",
        "print(f\"Root Mean Squared Error train : {train_RMSE}\")\n",
        "\n",
        "train_R2_score=r2_score(y_train,model.predict(x_train))\n",
        "print(f\"R2 Score train : {train_R2_score}\")\n",
        "\n",
        "train_adjusted_r2=1-(1-train_R2_score)*(len(y_train)-1)/(len(y_train)-x_train.shape[1]-1)\n",
        "print(f\"Adjusted R2 Score train : {train_adjusted_r2}\")\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print(' Test data metrics:')\n",
        "MAE=mean_absolute_error(y_test,y_pred)\n",
        "print(f\"Mean Absolute Error test : {MAE}\")\n",
        "\n",
        "MSE  = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE test :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE test :\" ,RMSE)\n",
        "\n",
        "R2_score=r2_score(y_test,y_pred)\n",
        "print(f\"R2 Score test : {R2_score}\")\n",
        "\n",
        "Adjusted_r2=1-(1-R2_score)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)\n",
        "print(f\"Adjusted R2 Score test : {Adjusted_r2}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jzH5aR0ckoJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression is a statistical method used in machine learning and data analysis that models the relationship between a dependent (target) variable and one or more independent (predictor) variables. The goal of regression is to predict the value of the dependent variable based on the values of the independent variables.\n",
        "\n",
        "performance of model:\n",
        "\n",
        "The MSE and RMSE value are lower as expected which results in good prediction by the model. The R2 scores and adjusted R2 are close to each other for both training and testing data.Overall, the model performs well on both the training and test sets, but there is a slightly higher level of error and slightly lower explanatory power on the test set, which is expected as the test set represents unseen data."
      ],
      "metadata": {
        "id": "ePzsL8MXh8Z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics = ['MAE','MSE', 'RMSE', 'R2', 'Adjusted R2 score']\n",
        "scores = [MAE,MSE,RMSE,R2_score,Adjusted_r2]\n",
        "\n",
        "# Plot the evaluation metric score chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel('Evaluation Metric')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Evaluation Metric Score Chart for Linear Regression ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visulaizing actual vs predicted data\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.lineplot(x=pred_data.index, y='Actual', data=pred_data, label='Actual')\n",
        "sns.lineplot(x=pred_data.index, y='Predicted', data=pred_data, label='Predicted')\n",
        "plt.title('Actual VS Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0HzeCLnbdNry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to implement RandomForest model.\n",
        "\n",
        "Random forest is an ensemble learning algorithm that constructs a multitude of decision trees at training time and outputs the mean prediction of the individual trees as the final prediction.\n",
        "\n",
        "The RandomForestRegressor class allows you to train a regression model using the random forest algorithm, and then use it to make predictions on new data.\n",
        "\n",
        "During the training process, the algorithm randomly selects a subset of features and a subset of observations to construct each decision tree.\n",
        "\n",
        "This helps to reduce overfitting and improve the generalization performance of the model."
      ],
      "metadata": {
        "id": "ErNQlYVoqfJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_rf =rf.predict(x_train)\n",
        "y_pred_test_rf =rf.predict(x_test)"
      ],
      "metadata": {
        "id": "2-ls9366qnvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "print('Training data metrics')\n",
        "MAE_train=mean_absolute_error(y_train,y_pred_train_rf)\n",
        "print(f\"Mean Absolute Error train : {MAE_train}\")\n",
        "\n",
        "MSE_train  = mean_squared_error(y_train, y_pred_train_rf)\n",
        "print(\"MSE train :\" , MSE_train)\n",
        "\n",
        "RMSE_train = np.sqrt(MSE_train)\n",
        "print(\"RMSE train :\" ,RMSE_train)\n",
        "\n",
        "R2_score_train=r2_score(y_train,y_pred_train_rf)\n",
        "print(f\"R2 Score train : {R2_score_train}\")\n",
        "\n",
        "Adjusted_r2_train=1-(1-R2_score_train)*(len(y_train)-1)/(len(y_train)-x_train.shape[1]-1)\n",
        "print(f\"Adjusted R2 Score train : {Adjusted_r2_train}\")\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('Metrics for testing data')\n",
        "MAE_test=mean_absolute_error(y_test,y_pred_test_rf)\n",
        "print(f\"Mean Absolute Error test : {MAE_test}\")\n",
        "\n",
        "MSE_test  = mean_squared_error(y_test, y_pred_test_rf)\n",
        "print(\"MSE test :\" , MSE_test)\n",
        "\n",
        "RMSE_test = np.sqrt(MSE_test)\n",
        "print(\"RMSE test :\" ,RMSE_test)\n",
        "\n",
        "R2_score_test=r2_score(y_test,y_pred_test_rf)\n",
        "print(f\"R2 Score test : {R2_score_test}\")"
      ],
      "metadata": {
        "id": "82cf14caqvfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be clearly seen that model is not performed well on unseen data indicating overfitting. Lets overcome that by tuning hyperparameter and using cross validation"
      ],
      "metadata": {
        "id": "-GNgudfmq_bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual VS predicted visulaization\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(((y_pred_test_rf)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kpA2brPhrBZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50,80,100,200,300],\n",
        "    'max_depth': [1,2,6,7,8,9,10,20,30,40],\n",
        "    'min_samples_split':[10,20,30,40,50,100,150,200],\n",
        "    'min_samples_leaf': [1,2,8,10,20,40,50]\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(rf, param_grid_rf,verbose=2, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best estimator\n",
        "best_model_rf_rs = random_search.best_estimator_\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Predict the model\n",
        "y_pred_train_rf_rs= random_search.predict(x_train)\n",
        "y_pred_test_rf_rs= random_search.predict(x_test)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics after hyperparameter tuning\n",
        "print('Metrics for training data after tuning')\n",
        "MAE_tain_ht=mean_absolute_error(y_train,y_pred_train_rf_rs)\n",
        "print(f'Mean absolute error: {MAE_tain_ht}')\n",
        "\n",
        "MSE_train_ht  = mean_squared_error(y_train, y_pred_train_rf_rs)\n",
        "print(\"MSE train :\" , MSE_train_ht)\n",
        "\n",
        "RMSE_train_ht = np.sqrt(MSE_train_ht)\n",
        "print(\"RMSE train :\" ,RMSE_train_ht)\n",
        "\n",
        "R2_score_train_ht=r2_score(y_train,y_pred_train_rf_rs)\n",
        "print(f\"R2 Score train : {R2_score_train_ht}\")\n",
        "\n",
        "Adjusted_r2_train_ht=1-(1-R2_score_train_ht)*(len(y_train)-1)/(len(y_train)-x_train.shape[1]-1)\n",
        "print(f\"Adjusted R2 Score train : {Adjusted_r2_train_ht}\")\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('Testing metrics after tuning')\n",
        "MAE_test_ht=mean_absolute_error(y_test,y_pred_test_rf_rs)\n",
        "print(f'Mean absolute error: {MAE_test_ht}')\n",
        "\n",
        "MSE_test_ht  = mean_squared_error(y_test, y_pred_test_rf_rs)\n",
        "print(\"MSE test :\" , MSE_test_ht)\n",
        "\n",
        "RMSE_test_ht = np.sqrt(MSE_test_ht)\n",
        "print(\"RMSE test :\" ,RMSE_test_ht)\n",
        "\n",
        "R2_score_test_ht=r2_score(y_test,y_pred_test_rf_rs)\n",
        "print(f\"R2 Score test : {R2_score_test_ht}\")\n",
        "\n",
        "adjusted_r2_test_ht=1-(1-R2_score_test_ht)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)\n",
        "print(f\"Adjusted R2 Score test : {adjusted_r2_test_ht}\")\n"
      ],
      "metadata": {
        "id": "95L1hojBxAMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual VS predicted visulaization after hyper parameter tuning\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(((y_pred_test_rf_rs)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n6Itvzbl3-9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: I had used Random serach CV as the hyperparameter tuning technique to adjust the hyperparamentes to get better reults."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: There is improvement in the model as the evaluation metris after the hyperparameter tuning came close together which shows that the model is perfomring well after the tuning."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: I considered R2. adjusted R2 and RMSE as evaluation metrics.\n",
        "\n",
        "R2 score is a measure of how well the model fits the data.\n",
        "\n",
        "It ranges from 0 to 1, with a higher value indicating a better fit which means that the model is able to explain a large portion of the variance in the data which could have a positive impact on decision-making.\n",
        "\n",
        "The adjusted R-squared provides a penalized measure of model fit that takes into account both the explanatory power of the model and the complexity of the model. It is particularly useful when comparing models with different numbers of predictors. A higher adjusted R-squared indicates a better fit of the model, as it reflects the proportion of variance in the dependent variable that is explained by the independent variables, adjusted for the model complexity.\n",
        "\n",
        "RMSE is a measure of the average squared error of the model's predictions.\n",
        "\n",
        "It is calculated as the square root of the mean squared error (MSE).\n",
        "\n",
        "In a business context, a low RMSE can indicate that the model is making relatively small errors."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer; I choose RandomForest as my future as it has predeicted the results more close to the actual value.his model has the highest R2 and adjusted r2 values on both the training and test sets, which indicates that it is doing a good job of explaining the variance in the target variable and also considering all the features."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        " **Random Forest**\n",
        " Random Forest is an ensemble learning method that constructs a multitude of decision trees during training. Each tree in the forest operates independently and makes a prediction. The final prediction of the random forest is typically the mode (for classification) or the average prediction (for regression) of the individual trees.\n",
        "\n",
        "**Advantages**:\n",
        "                      \n",
        " Feature Importance in Random Forest is determined based on how much each feature decreases the impurity in the model (measured by Gini impurity for classification and MSE for regression) across all trees in the forest. Features that contribute more to reducing impurity are considered more important.\n",
        "\n",
        "Using Model Explainability Tools:\n",
        "\n",
        "Feature Importance Plot: This plot visualizes the importance of each feature in the model.\n",
        "Permutation Importance: Computes the increase in model prediction error after permuting (shuffling) the feature's values, indicating how much the model relies on that feature.\n",
        "Partial Dependence Plots (PDP): Shows the relationship between a feature and the predicted outcome, accounting for the average effects of all other features."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main goal of the project is to create a machine learning model which can predict the closing price of Yes Bank stock for that month, keeping in mind of the fraud case happened in 2018.\n",
        "\n",
        "I have developed 2 models Linear Regression and  Random forest. RandomForest model shows promising result with R2 score of 0.97 on train and 0.95 on test dataset, therefore it can be used to solve this problem. It also considering all newlt added features and taking care of multicollinearity.\n",
        "\n",
        "Using data visualization on our target variable, we can clearly see the impact of 2018 fraud case involving Rana Kapoor as the stock prices decline dramatically during that period.\n",
        "\n",
        "We found that the distribution of all our variables is positively skewed. so we performed log transformation on them.\n",
        "\n",
        "Additional features were engineered by taking mean of Open, High and Low as mean ohl as our new feature.\n",
        "\n",
        "The feautures which plays a crucial role in deciding closing price is the mean_ohl.\n",
        "\n",
        "Given the dataset and features, Our model is performing well on all data-points. With our model making predictions with such high accuracy, we can confidently deploy this model for further predictive tasks using future data."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}